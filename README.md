# Model Optimization

Optimising ML Models for efficient deployment, especially on systems without GPUs involves several key techniques. This notebook will cover the following:

1. Model Quantization
2. Model Pruning
3. Knowledge Distillation


## Model Quantization

Post-Training Quantization (PTQ)

Quantization-Aware Training (QAT)

## Model Pruning

Weight Pruning (Unstructured Pruning)

Neuron Pruning (Structured Pruning)

Layer Pruning

## Knowledge Distillation

Response-Based Knowledge Distillation (Logit Distillation)

Feature-Based Knowledge Distillation (Intermediate Representation Transfer)

Relation-Based Knowledge Distillation

Self-Knowledge Distillation

Cross-Model Knowledge Distillation

Online Knowledge Distillation

Multi-Teacher Knowledge Distillation